{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d31ad6-9abd-49ff-a593-b3c5de934924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e7b759f-4f10-477e-9148-9a783546cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--dataset\",\n",
    "    type=str,\n",
    "    default=\"training_sequences_noC.csv\",\n",
    "    help=\"dataset file for training, a csv file\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--random_seed\", type=int, default=123, help=\"rando seed for reproducibility\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=64, help=\"the batch size to update network\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--block_size\",\n",
    "    type=int,\n",
    "    default=40,\n",
    "    help=\"portion of characters used for training batch\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--vocab_size\",\n",
    "    type=int,\n",
    "    default=20,\n",
    "    help=\"number of unique characters or words used used as input features\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--embed_dim\",\n",
    "    type=int,\n",
    "    default=100,\n",
    "    help=\"number of embedding demensions aka dimension vectors to represent entire vocab as input features\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_size\",\n",
    "    type=int,\n",
    "    default=200,\n",
    "    help=\"number of neurons to be projects from the input feature size\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_layers\", type=int, default=2, help=\"number of hidden layers\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_size\",\n",
    "    type=int,\n",
    "    default=20,\n",
    "    help=\"number of unique characters or words as output features\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dropout_rate\",\n",
    "    type=float,\n",
    "    default=0.5,\n",
    "    help=\"probability of an element to be zeroed in dropout layer\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--l2_reg\",\n",
    "    type=float,\n",
    "    default=0.03,\n",
    "    help=\"l2 regularization coefficient\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gamma\",\n",
    "    type=float,\n",
    "    default=0.59,\n",
    "    help=\"gamma for learning rate scheduler\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--momentum\",\n",
    "    type=float,\n",
    "    default=0.5,\n",
    "    help=\"momentum for optimizer learning rate used to update weights\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--alpha\",\n",
    "    type=float,\n",
    "    default=0.79,\n",
    "    help=\"alpha for optimizer learning rate used to update weights\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--epsilon\",\n",
    "    type=float,\n",
    "    default=1e-08,\n",
    "    help=\"epsilon for optimizer learning rate used to update weights\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    type=int,\n",
    "    default=8,\n",
    "    help=\"number of steps to accumulate gradients before performing a backward/update pass\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--betas\",\n",
    "    type=float,\n",
    "    default=(0.9, 0.999),\n",
    "    help=\"beta1 for optimizer learning rate used to update weights\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--step_size\",\n",
    "    type=float,\n",
    "    default=50,\n",
    "    help=\"step size for learning rate scheduler to decay learning rate\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weight_decay\",\n",
    "    type=float,\n",
    "    default=0.001,\n",
    "    help=\"weight decay for optimizer learning rate used to update weights\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",\n",
    "    type=int,\n",
    "    default=50,\n",
    "    help=\"number of epochs. Number of full passes through the training examples.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--learning_rate\",\n",
    "    type=float,\n",
    "    default=0.01,\n",
    "    help=\"learning rate for model training\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--k_folds\",\n",
    "    type=int,\n",
    "    default=2,\n",
    "    help=\"cross validation folds for training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--loss_plot_name\",\n",
    "    type=str,\n",
    "    default=\"loss.png\",\n",
    "    help=\"loss plot name to save the plot\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--accuracy_plot_name\",\n",
    "    type=str,\n",
    "    default=\"accuracy.png\",\n",
    "    help=\"accuracy plot name to save the plot\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--sampled_text_name\",\n",
    "    type=str,\n",
    "    default=\"sampled_text.txt\",\n",
    "    help=\"sampled text name to save the text\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mode\",\n",
    "    choices=[\"pretrain\", \"cross_validate\", \"finetune\", \"sample\"],\n",
    "    default=\"pretrain\",\n",
    "    help=\"Mode: pretrain, cross-validate, finetune, sample\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--session_name\",\n",
    "    choices=[\"pretrain\", \"crossval\", \"finetune\", \"sample\"],\n",
    "    default=\"pretrain\",\n",
    "    help=\"Mode: pretrain, cross-validate, finetune, sample\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name\",\n",
    "    type=str,\n",
    "    default=\"model\",\n",
    "    help=\"model name to save the model\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--start_char\",\n",
    "    type=str,\n",
    "    default=\"B\",\n",
    "    help=\"start character to begin sampling\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--sample_len\",\n",
    "    type=int,\n",
    "    default=100,\n",
    "    help=\"number of sequences to sample training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--temp\",\n",
    "    type=float,\n",
    "    default=1.0,\n",
    "    help=\"temperature used to sample text\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--checkpoint\",\n",
    "    type=str,\n",
    "    # default=None,\n",
    "    help=\"filename of the pretrained model to used for sampling if train=False\",\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7cd4450-0a4d-41f9-aeda-f9a4b3523db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(args.random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def reset_random_seeds(seed):\n",
    "    \"\"\"\n",
    "    reset random seeds for reproducibility\n",
    "    :param seed: {int} random seed\n",
    "    Returns:\n",
    "        -None\n",
    "    \"\"\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "reset_random_seeds(args.random_seed)\n",
    "\n",
    "\n",
    "def load_text(file):\n",
    "    \"\"\"\n",
    "    load text from a file\n",
    "    :param file: {str} file of the text file to be read\n",
    "    Returns:\n",
    "        -text : {str} text\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file, \"r\", encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        # uppercase all text\n",
    "        text = text.upper()\n",
    "    return text\n",
    "\n",
    "\n",
    "ROOT_DIR = \"../data/training_sequences_noC.csv\"\n",
    "text = load_text(ROOT_DIR)\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "vocab = \"\".join(chars)\n",
    "\n",
    "\n",
    "def string_to_int(text):\n",
    "    \"\"\"\n",
    "    encode a given text/char into integers (pytorch tensors)\n",
    "    :param text: {str} text to be encoded\n",
    "    Returns:\n",
    "        -tensor: {torch.tensor} encoded text\n",
    "    \"\"\"\n",
    "    chars = sorted(list(set(text)))\n",
    "    stoi = {cha: i for i, cha in enumerate(chars)}\n",
    "    encode = [stoi[cha] for cha in text]\n",
    "    tensor = torch.tensor(encode).long()\n",
    "    return tensor\n",
    "\n",
    "\n",
    "split_idx = int(0.8 * len(text))\n",
    "train_data = text[:split_idx]\n",
    "val_data = text[split_idx:]\n",
    "\n",
    "\n",
    "def text_chunks(text):\n",
    "    \"\"\"\n",
    "    create text chunks consisting of [text length] number of character\n",
    "    each. They will then be used to construct input and targert text, both\n",
    "    with [text length] number of elements.\n",
    "    param: text: {str} text to be chunked\n",
    "    Returns:\n",
    "        -text_chunks: {list} list of text length\n",
    "    \"\"\"\n",
    "    block_size = args.block_size + 1\n",
    "    encoded_text = string_to_int(text)\n",
    "    text_chunks = [\n",
    "        encoded_text[i : i + block_size]\n",
    "        for i in range(len(encoded_text) - block_size + 1)\n",
    "    ]\n",
    "    random.shuffle(text_chunks)\n",
    "    return text_chunks\n",
    "\n",
    "\n",
    "train_chunks = text_chunks(train_data)\n",
    "val_chunks = text_chunks(val_data)\n",
    "\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    create a dataset of text\n",
    "    Attributes: chunks: {list} list of text chunks\n",
    "    Returns:\n",
    "        -inpt: {torch.tensor} input text\n",
    "        -target: {torch.tensor} target text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chunks):\n",
    "        self.chunks = chunks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunks = self.chunks[idx]\n",
    "        inpt = chunks[:-1].long()\n",
    "        target = chunks[1:].long()\n",
    "        return inpt, target\n",
    "\n",
    "\n",
    "train_dataset = TextDataset(train_chunks)\n",
    "val_dataset = TextDataset(val_chunks)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c65edb8b-f727-4954-bb6d-4fd6e451961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_dl,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    scheduler,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "    block_size,\n",
    "):\n",
    "    \"\"\"\n",
    "    train the model\n",
    "    :param model: {torch.nn.Module} model to be trained\n",
    "    :param train_dl: {torch.utils.data.DataLoader} training data loader\n",
    "    :param optimizer: {torch.optim} optimizer\n",
    "    :param criterion: {torch.nn} loss function\n",
    "    :param scheduler: {torch.optim.lr_scheduler} learning rate scheduler\n",
    "    :param weight_decay: {float} weight decay\n",
    "    :param batch_size: {int} batch size\n",
    "    :param block_size: {int} block size\n",
    "    Returns:\n",
    "        -train_loss: {float} training loss\n",
    "        -train_acc: {float} training accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_acc = 0\n",
    "\n",
    "    text_batch, target_batch = next(iter(train_dl))\n",
    "    # for text_batch, target_batch in train_dl:\n",
    "    text_batch.to(device)\n",
    "    target_batch.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    hidden, cell = model.init_hidden(batch_size)\n",
    "    for c in range(block_size):\n",
    "        pred, hidden, cell = model(text_batch[:, c], hidden, cell)\n",
    "        loss += criterion(pred, target_batch[:, c])\n",
    "\n",
    "        # L2 regularization\n",
    "        l2_loss = 0.0\n",
    "        for param in model.parameters():\n",
    "            l2_loss += torch.norm(param, p=2)\n",
    "        loss += weight_decay * l2_loss\n",
    "\n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "    loss = loss.item() / block_size\n",
    "    train_running_loss += loss\n",
    "    # accuracy\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    target = target_batch[:, c]\n",
    "    acc = (pred == target).sum().item() / len(target)\n",
    "    train_running_acc += acc\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_running_loss, train_running_acc\n",
    "\n",
    "\n",
    "def validate(model, val_dl, optimizer, criterion, batch_size, block_size):\n",
    "    \"\"\"\n",
    "    validate the model\n",
    "    :param model: {torch.nn.Module} model to be trained\n",
    "    :param val_dl: {torch.utils.data.DataLoader} validation data loader\n",
    "    :param optimizer: {torch.optim} optimizer\n",
    "    :param criterion: {torch.nn} loss function\n",
    "    :param batch_size: {int} batch size\n",
    "    :param block_size: {int} block size\n",
    "    Returns:\n",
    "        -val_loss: {float} validation loss\n",
    "        -val_acc: {float} validation accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_running_loss = 0.0\n",
    "        val_running_acc = 0\n",
    "\n",
    "        text_batch, target_batch = next(iter(val_dl))\n",
    "        # for text_batch, target_batch in val_dl:\n",
    "        text_batch.to(device)\n",
    "        target_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        # forward pass\n",
    "        hidden, cell = model.init_hidden(batch_size)\n",
    "        for c in range(block_size):\n",
    "            pred, hidden, cell = model(text_batch[:, c], hidden, cell)\n",
    "            loss += criterion(pred, target_batch[:, c])\n",
    "        loss = loss.item() / block_size\n",
    "        val_running_loss += loss\n",
    "        # accuracy\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        target = target_batch[:, c]\n",
    "        acc = (pred == target).sum().item() / len(target)\n",
    "        val_running_acc += acc\n",
    "\n",
    "    return val_running_loss, val_running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eea098da-66d7-475a-b598-149fd3230ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        output_size,\n",
    "        dropout_rate,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "        :param vocab_size {int}: size of vocabulary\n",
    "        :param embed_dim {int}: embedding dimension\n",
    "        :param hidden_size {int}: hidden size\n",
    "        :param num_layers {int}: number of layers\n",
    "        :param output_size {int}: output size\n",
    "        :param dropout_rate {float}: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc1 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, character, hidden, cell):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        :param character: input character\n",
    "        :param hidden: hidden state\n",
    "        :param cell: cell state\n",
    "        Returns:\n",
    "            output (torch.Tensor): output\n",
    "            hidden (torch.Tensor): hidden state\n",
    "            cell (torch.Tensor): cell state\n",
    "        \"\"\"\n",
    "\n",
    "        output = self.embedding(character).unsqueeze(\n",
    "            1\n",
    "        )  # reshape to batch_size * 1 * embed_dim\n",
    "        output, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
    "        output = self.dropout(output)  # applying dropout to the output\n",
    "        output = F.relu(output)\n",
    "        output = self.fc1(output).reshape(\n",
    "            output.size(0), -1\n",
    "        )  # reshape to batch_size * output_size\n",
    "\n",
    "        return output, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize hidden state\n",
    "        :param batch_size: batch size\n",
    "        Returns:\n",
    "            hidden (torch.Tensor): hidden state\n",
    "            cell (torch.Tensor): cell state\n",
    "        \"\"\"\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112dfdb-6dc2-43b8-89d8-9ff216527de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA STATISTICS\n",
      "\n",
      "Total samples: 1,554\n",
      "Mean sample length: 20.82\n",
      "Standard deviation: 7.72\n",
      "Max sample length: 48\n",
      "Min sample length: 7\n",
      "Vocabulary size: 20\n",
      "Vocabulary: \n",
      "ADEFGHIKLMNPQRSTVWY\n",
      "================================================================================\n",
      "\n",
      "TRAINING INFO\n",
      "\n",
      "Computing on: cpu device\n",
      "Model architecture: LSTM(\n",
      "  (embedding): Embedding(20, 100)\n",
      "  (lstm): LSTM(100, 200, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=200, out_features=20, bias=True)\n",
      ")\n",
      "Total samples: 33,913\n",
      "Total training samples: 27,130\n",
      "Total validation samples: 6,783\n",
      "Total parameters: 569,220\n",
      "Training parameters: 569,220\n",
      "================================================================================\n",
      "\n",
      "CROSS VALIDATION\n",
      "\n",
      "HYPERPARAMETER SEARCH\n",
      "\n",
      "Total folds: 2\n",
      "Total hyperparameter combinations: 8\n",
      "Total fits: 16\n",
      "================================================================================\n",
      "\n",
      "Fold 1 of 2.\n",
      "Candidate 1 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 1, hidden: 100, Dropout: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8f0468cbf64e5cadedeb5b8f3477d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.424 at epoch: 49.000\n",
      "Time elapsed: 0.25 min\n",
      "================================================================================\n",
      "\n",
      "Fold 1 of 2.\n",
      "Candidate 2 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 1, hidden: 100, Dropout: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f1a6b8630848d7b5eca0ca37ec27b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.436 at epoch: 49.000\n",
      "Time elapsed: 0.52 min\n",
      "================================================================================\n",
      "\n",
      "Fold 1 of 2.\n",
      "Candidate 3 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 1, hidden: 200, Dropout: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262b0b0189994eb392fc9c6544768729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.467 at epoch: 49.000\n",
      "Time elapsed: 0.81 min\n",
      "================================================================================\n",
      "\n",
      "Fold 1 of 2.\n",
      "Candidate 4 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 1, hidden: 200, Dropout: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b24f4d9d8b43f0affe21e88e972ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.459 at epoch: 45.000\n",
      "Time elapsed: 1.09 min\n",
      "================================================================================\n",
      "\n",
      "Fold 1 of 2.\n",
      "Candidate 5 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 2, hidden: 100, Dropout: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6937d2690e214b5080c18edb62687e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.603 at epoch: 49.000\n",
      "Time elapsed: 1.55 min\n",
      "================================================================================\n",
      "\n",
      "Fold 1 of 2.\n",
      "Candidate 6 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 2, hidden: 100, Dropout: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c7ae4c728841fcbef9b02101ddb81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.634 at epoch: 48.000\n",
      "Time elapsed: 2.03 min\n",
      "================================================================================\n",
      "\n",
      "Fold 1 of 2.\n",
      "Candidate 7 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 2, hidden: 200, Dropout: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5dd2f4a85843588b808bb70864c95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.643 at epoch: 45.000\n",
      "Time elapsed: 2.54 min\n",
      "================================================================================\n",
      "\n",
      "Fold 1 of 2.\n",
      "Candidate 8 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 2, hidden: 200, Dropout: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a861eb7f7f4be3b8b92225d476871b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.706 at epoch: 43.000\n",
      "Time elapsed: 3.03 min\n",
      "================================================================================\n",
      "================================================================================\n",
      "\n",
      "Best Fold 1 Hyperparameters:\n",
      "{'layers': 1, 'hidden': 100, 'dropout': 0.1}\n",
      "Best Loss: 1.424 at epoch: 49.000\n",
      "================================================================================\n",
      "================================================================================\n",
      "\n",
      "Fold 2 of 2.\n",
      "Candidate 1 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 1, hidden: 100, Dropout: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3bb274c613f40d78d8fb889976a8573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.518 at epoch: 43.000\n",
      "Time elapsed: 3.33 min\n",
      "================================================================================\n",
      "\n",
      "Fold 2 of 2.\n",
      "Candidate 2 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 1, hidden: 100, Dropout: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2196a108a8674040a2091465d9e82044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.397 at epoch: 47.000\n",
      "Time elapsed: 3.59 min\n",
      "================================================================================\n",
      "\n",
      "Fold 2 of 2.\n",
      "Candidate 3 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 1, hidden: 200, Dropout: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1828b3b4dbd5418d825270bcb1e44cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.484 at epoch: 49.000\n",
      "Time elapsed: 3.87 min\n",
      "================================================================================\n",
      "\n",
      "Fold 2 of 2.\n",
      "Candidate 4 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 1, hidden: 200, Dropout: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2fd83a7991457aba8e5af85498e957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.455 at epoch: 46.000\n",
      "Time elapsed: 4.15 min\n",
      "================================================================================\n",
      "\n",
      "Fold 2 of 2.\n",
      "Candidate 5 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 2, hidden: 100, Dropout: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98472c3d4ed84e46ac552440ed225b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.597 at epoch: 49.000\n",
      "Time elapsed: 4.65 min\n",
      "================================================================================\n",
      "\n",
      "Fold 2 of 2.\n",
      "Candidate 6 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 2, hidden: 100, Dropout: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a89669615ad4dc4b16e808958fad81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.583 at epoch: 45.000\n",
      "Time elapsed: 5.15 min\n",
      "================================================================================\n",
      "\n",
      "Fold 2 of 2.\n",
      "Candidate 7 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 2, hidden: 200, Dropout: 0.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861140cfefc44aa0bf39d240238c8dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.510 at epoch: 44.000\n",
      "Time elapsed: 5.67 min\n",
      "================================================================================\n",
      "\n",
      "Fold 2 of 2.\n",
      "Candidate 8 of 8.\n",
      "Hyperparameters:\n",
      "Layers: 2, hidden: 200, Dropout: 0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc84f6eb3d74b1b9f9ec94fb1133a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/50 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss: 1.629 at epoch: 42.000\n",
      "Time elapsed: 6.14 min\n",
      "================================================================================\n",
      "================================================================================\n",
      "\n",
      "Best Fold 2 Hyperparameters:\n",
      "{'layers': 1, 'hidden': 100, 'dropout': 0.2}\n",
      "Best Loss: 1.397 at epoch: 47.000\n",
      "================================================================================\n",
      "================================================================================\n",
      "================================================================================\n",
      "\n",
      "Best Hyperparameters for 2 folds and 16 fits :\n",
      "{'layers': 1, 'hidden': 100, 'dropout': 0.2}\n",
      "Best Loss: 1.397 at epoch: 47.000\n",
      "Time elapsed: 6.14 min\n",
      "================================================================================\n",
      "================================================================================\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def main():\n",
    "    reset_random_seeds(args.random_seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(\"\\nDATA STATISTICS\\n\")\n",
    "    print(f\"Total samples: {len(text.split()):,}\")\n",
    "    print(f\"Mean sample length: {np.mean([len(s) for s in text.split()]):.2f}\")\n",
    "    print(f\"Standard deviation: {np.std([len(s) for s in text.split()]):.2f}\")\n",
    "    print(f\"Max sample length: {np.max([len(s) for s in text.split()]):,}\")\n",
    "    print(f\"Min sample length: {np.min([len(s) for s in text.split()]):,}\")\n",
    "    print(f\"Vocabulary size: {len(chars):,}\")\n",
    "    print(f\"Vocabulary: {vocab}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    model = LSTM(\n",
    "        vocab_size=args.vocab_size,\n",
    "        embed_dim=args.embed_dim,\n",
    "        hidden_size=args.hidden_size,\n",
    "        num_layers=args.num_layers,\n",
    "        output_size=args.vocab_size,\n",
    "        dropout_rate=args.dropout_rate,\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.RMSprop(\n",
    "        model.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "        alpha=args.alpha,\n",
    "        eps=args.epsilon,\n",
    "        momentum=args.momentum,\n",
    "        centered=False,\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=args.step_size, gamma=args.gamma\n",
    "    )\n",
    "\n",
    "    print(\"\\nTRAINING INFO\\n\")\n",
    "    print(f\"Computing on: {device} device\")\n",
    "    print(f\"Model architecture: {model}\")\n",
    "    print(f\"Total samples: {len(text):,}\")\n",
    "    print(f\"Total training samples: {len(train_data):,}\")\n",
    "    print(f\"Total validation samples: {len(val_data):,}\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    total_trainable_params = sum(\n",
    "        p.numel() for p in model.parameters() if p.requires_grad\n",
    "    )\n",
    "    print(f\"Training parameters: {total_trainable_params:,}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(\"\\nCROSS VALIDATION\")\n",
    "\n",
    "    train_dl = train_dataloader\n",
    "    train_loss, train_acc = [], []\n",
    "    val_loss, val_acc = [], []\n",
    "\n",
    "    k = args.k_folds\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    hidden = [100, 200]\n",
    "    layers = [1, 2]\n",
    "    dropout = [0.1, 0.2]\n",
    "\n",
    "    hyperparam_combinations = list(\n",
    "        itertools.product(layers, hidden, dropout)\n",
    "    )\n",
    "    total_combinations = len(hyperparam_combinations)\n",
    "    total_fits = total_combinations * k\n",
    "\n",
    "    print(\"\\nHYPERPARAMETER SEARCH\\n\")\n",
    "    print(f\"Total folds: {k}\")\n",
    "    print(f\"Total hyperparameter combinations: {total_combinations}\")\n",
    "    print(f\"Total fits: {total_fits}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    all_best_loss = np.inf\n",
    "    all_best_epoch = 0\n",
    "    all_best_hyperparams = {}\n",
    "\n",
    "    for fold, (_, _) in enumerate(kf.split(train_dl.dataset)):\n",
    "        best_hyperparams = {}\n",
    "        fold_best_loss = np.inf\n",
    "        fold_best_epoch = 0\n",
    "        for combination in hyperparam_combinations:\n",
    "            l, h, d = combination\n",
    "            print(f\"\\nFold {fold + 1} of {k}.\")\n",
    "            print(\n",
    "                f\"Candidate {hyperparam_combinations.index(combination) + 1} of {total_combinations}.\"\n",
    "            )\n",
    "            print(\"Hyperparameters:\")\n",
    "            print(f\"Layers: {l}, hidden: {h}, Dropout: {d}\")\n",
    "\n",
    "            model = LSTM(\n",
    "                vocab_size=args.vocab_size,\n",
    "                embed_dim=args.embed_dim,\n",
    "                hidden_size=args.hidden_size,\n",
    "                num_layers=l,\n",
    "                output_size=args.vocab_size,\n",
    "                dropout_rate=d,\n",
    "            )\n",
    "\n",
    "            model = model\n",
    "            model = model.to(device)\n",
    "            optimizer = optim.RMSprop(\n",
    "                model.parameters(),\n",
    "                lr=args.learning_rate,\n",
    "                weight_decay=args.weight_decay,\n",
    "                alpha=args.alpha,\n",
    "                eps=args.epsilon,\n",
    "                momentum=args.momentum,\n",
    "                centered=False,\n",
    "            )\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            scheduler = optim.lr_scheduler.StepLR(\n",
    "                optimizer, step_size=args.step_size, gamma=args.gamma\n",
    "            )\n",
    "\n",
    "            best_loss = np.inf\n",
    "            best_epoch = 0\n",
    "            # for step, (_, _) in enumerate(train_dataloader):\n",
    "            for epoch in tqdm(\n",
    "                range(args.num_epochs),\n",
    "                desc=\"Epochs\",\n",
    "                unit=\"epoch\",\n",
    "                leave=True,\n",
    "                position=0,\n",
    "            ):\n",
    "                train_epoch_loss, train_epoch_acc = train(\n",
    "                    model=model,\n",
    "                    train_dl=train_dataloader,\n",
    "                    optimizer=optimizer,\n",
    "                    criterion=criterion,\n",
    "                    scheduler=scheduler,\n",
    "                    weight_decay=args.weight_decay,\n",
    "                    batch_size=args.batch_size,\n",
    "                    block_size=args.block_size,\n",
    "                )\n",
    "\n",
    "                val_epoch_loss, val_epoch_acc = validate(\n",
    "                    model=model,\n",
    "                    val_dl=val_dataloader,\n",
    "                    optimizer=optimizer,\n",
    "                    criterion=criterion,\n",
    "                    batch_size=args.batch_size,\n",
    "                    block_size=args.block_size,\n",
    "                )\n",
    "\n",
    "                train_loss.append(train_epoch_loss)\n",
    "                train_acc.append(train_epoch_acc)\n",
    "                val_loss.append(val_epoch_loss)\n",
    "                val_acc.append(val_epoch_acc)\n",
    "\n",
    "                # print(\n",
    "                #     f\"Epoch: {epoch:03d} | \"\n",
    "                #     f\"Val Acc : {val_epoch_acc:.3f} | \"\n",
    "                #     f\"Val Loss: {val_epoch_loss:.3f}\"\n",
    "                # )\n",
    "\n",
    "                # get the best hyperparameters\n",
    "                if val_epoch_loss < best_loss:\n",
    "                    best_loss = val_epoch_loss\n",
    "                    best_epoch = epoch\n",
    "\n",
    "            # save fold best loss\n",
    "            if best_loss < fold_best_loss:\n",
    "                fold_best_loss = best_loss\n",
    "                fold_best_epoch = best_epoch\n",
    "                best_hyperparams = {\n",
    "                    \"layers\": l,\n",
    "                    \"hidden\": h,\n",
    "                    \"dropout\": d,\n",
    "                }\n",
    "\n",
    "            # Print the best loss\n",
    "            print(f\"Best Loss: {best_loss:.3f} at epoch: {best_epoch:.3f}\")\n",
    "            print(f\"Time elapsed: {(time.time() - start_time) / 60:.2f} min\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "        # save all best loss\n",
    "        if fold_best_loss < all_best_loss:\n",
    "            all_best_loss = fold_best_loss\n",
    "            all_best_epoch = fold_best_epoch\n",
    "            all_best_hyperparams = best_hyperparams\n",
    "\n",
    "        # Print the best hyperparameters per epoch\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nBest Fold {fold + 1} Hyperparameters:\")\n",
    "        print(best_hyperparams)\n",
    "        print(f\"Best Loss: {fold_best_loss:.3f} at epoch: {fold_best_epoch:.3f}\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    # Print the best hyperparameters for all folds\n",
    "    print(\"=\" * 80)\n",
    "    # overall best hyperparameters\n",
    "    print(f\"\\nBest Hyperparameters for {k} folds and {total_fits} fits :\")\n",
    "    print(all_best_hyperparams)\n",
    "    print(f\"Best Loss: {all_best_loss:.3f} at epoch: {all_best_epoch:.3f}\")\n",
    "    print(f\"Time elapsed: {(time.time() - start_time)/60:.2f} min\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Save the best hyperparameters to a file\n",
    "    with open(\"../reports/best_hyperparams.txt\", \"w\") as f:\n",
    "        f.write(f\"\\nBest Hyperparameters for {k} folds and {total_fits}fits:\\n\")\n",
    "        f.write(str(all_best_hyperparams))\n",
    "        f.write(f\"\\nBest Loss: {all_best_loss:.3f}\")\n",
    "        f.write(f\"\\nBest Epoch: {all_best_epoch:.3f}\")\n",
    "        f.write(f\"\\nTime elapsed: {(time.time() - start_time)/60:.2f} min\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19cb0c-273a-4e56-b562-d758e7e7cb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
